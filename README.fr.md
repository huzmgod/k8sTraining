# k8sFormation

Apache, Nginx, Jenkins, Istio, Kiali (pour le moment)

V√©rifier**_TrainingPathProject.pdf_**pour un fichier Lisez-moi complet üòä

Pour d√©ployer l'ensemble du sc√©nario, ex√©cutez¬†:

```bash
./autoDeploy.sh
```

Si vous pr√©f√©rez jouer lentement, il suffit de¬†:

```bash
git clone https://github.com/huzmgod/k8sTraining.git
cd k8sTraining
```

**_-----------------------------------------------------------------------------------------------------------------------_**

**_README complet en espagnol (sans les images, qui sont dans le pdf)¬†:_**

**Configuration de Nginx en tant que proxy inverse pour Apache √† l'aide de K8. Jenkins, Istio, K8s Dashboard et Kiali d√©ploy√©s**

1.  **Type d'installation¬†:<https://kind.sigs.k8s.io/docs/user/quick-start/#installation>**
2.  **Cr√©ez le fichier de configuration du cluster (cluster-config.yaml)¬†:**

cluster-config.yaml

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
 extraPortMappings:
 - containerPort: 30000
   hostPort: 30000
   protocol: TCP
```

**NOTES¬†:**

-   L'extraPortMappings est important pour les environnements qui utilisent Docker Desktop, car sinon la configuration kind emp√™che l'acc√®s ult√©rieur au contenu du serveur Apache (ou autre) via localhost:30000 (ou un autre port sup√©rieur √† 30000). Pour plus de documentation¬†:<https://kind.sigs.k8s.io/docs/user/configuration/#extra-port-mappings>\*\*

-   Les travailleurs pourraient √™tre ajout√©s avec la ligne

```yaml
-role: worker
```

3.  **Cr√©ez le cluster et v√©rifiez l'√©tat¬†:**

```bash
kind create cluster --config=cluster kubectl cluster-info
kubectl cluster-info
```

4.  **Fichiers de configuration des pods (Apache et Nginx)¬†:**

**UTILISER:**Le service et le d√©ploiement sont dans le m√™me yaml, √† enregistrer sur kubectl apply -f. Vous pouvez √©galement utiliser kustomize pour gagner du temps de d√©ploiement.

**nginx.yaml¬†:**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30000
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nginx-conf
          mountPath: /etc/nginx/
      volumes:
      - name: nginx-conf
        configMap:
          name: nginx-conf
```

apache.yaml¬†:

```yaml

apiVersion: v1
kind: Service
metadata:
  name: apache
  labels:
    app: apache
spec:
  ports:
  - name: http
    port: 8080
    targetPort: 80
  selector:
    app: apache
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apache
spec:
  selector:
    matchLabels:
      app: apache
  replicas: 1
  template:
    metadata:
      labels:
        app: apache
    spec:
      containers:
      - name: apache
        image: httpd:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: html
          mountPath: /usr/local/apache2/htdocs/
      volumes:
      - name: html
        configMap:
          name: html
```

5.  **Cr√©ez le ConfigMap pour configurer nginx en tant que proxy inverse (nginx-conf.yaml)**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-conf
data:
  nginx.conf: |
    events {
      worker_connections  1024;
    }

    http {
      server {
        listen 80;
        server_name localhost;

        location / {
          index index.html;
          proxy_pass http://apache:8080/;
          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }
      }
    }
```

**NOTES¬†:**

-   _La directive sur les travailleurs_connexions contr√¥le le nombre maximum de connexions simultan√©es pouvant √™tre g√©r√©es par un processus de travail Nginx. Il doit √™tre adapt√© au trafic, 1024 en est un exemple._
-   _Le port Apache 80 est mapp√© sur 8080 pour √©viter les conflits avec le pod nginx._

6.  **Appliquez les configurations au cluster¬†:**

```bash
kubectl apply -f nginx.yaml
kubectl apply -f apache.yaml
kubectl apply -f nginx-conf.yaml
```

On v√©rifie que tout va bien

```bash
kubectl get pods
kubectl exec <nombre de tu pod de nginx> -- cat /etc/nginx/nginx.conf
```

7.  **Si nous avons du HTML personnalis√©, nous pouvons l'utiliser pour le visualiser via nginx et nous assurer que tout fonctionne bien jusqu'√† pr√©sent.**

_Les fichiers que je laisse s'appellent custom.html et custom.css, mais ils peuvent √™tre n'importe quel autre. C'est un HTML Dragon Ball qui est plut√¥t cool._

-   **Nous cr√©ons le ConfigMap √† partir des fichiers html et css¬†:**

```bash
kubectl create configmap html --from-file=custom.html --from-file=custom.css
```

REMARQUE¬†: Le fichier html.yaml est √©galement laiss√© en tant que ConfigMap √† appliquer avec kubectl

```bash
apply -f html.yaml
```

_C'est √©quivalent √† la commande pr√©c√©dente._

-   **Nous mettons √† jour le fichier apache.yaml, en cr√©ant un point de montage et en sp√©cifiant le volume √† utiliser¬†:**

```yaml
...
spec:
      containers:
      - name: apache
        image: httpd:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: html
          mountPath: /usr/local/apache2/htdocs/
      volumes:
      - name: html
        configMap:
          name: html
```

-   **Nous appliquons les configurations :**

```bash
kubectl apply -f apache.yaml
```

-   **On v√©rifie que tout a √©t√© charg√© et que le reverse proxy fonctionne correctement :**

```bash
kubectl exec <nombre de tu pod de nginx> -- curl localhost/custom.html
```

8.  **√Ä pr√©sent, nous devrions avoir configur√© correctement cela.**

**Et via localhost:30000, nous acc√©derions √† notre page Web custom.html**

9.  **Si tout s‚Äôest bien pass√© jusqu‚Äô√† pr√©sent, tant mieux, c‚Äôest comme √ßa que √ßa devrait √™tre. Sinon, j'ai laiss√© un fichier kustomization.yaml qui utilise kustomize pour tout construire automatiquement. Si quelque chose ne va pas, suivez les √©tapes suivantes¬†:**

```bash
kind delete cluster ‚Äì-name=<nombreDeTuCluster>
kind create cluster --config=cluster-config.yaml --name=<nombre-que-quieras>
kubectl apply -k .
```

**Tout pr√™t. Effectuez les v√©rifications ci-dessus (nginx.conf, localhost:30000, etc.) REMARQUE¬†:**

-   **Attention au copier-coller, les scripts ne se copient pas toujours bien.**
-   **Je sais que √ßa aurait √©t√© bien plus t√¥t, mais il faut tout reprendre depuis le d√©but**üòä**.**

**---------------------------------- TABLEAU DE BORD K8S -------------- --------------------------------**

**Ce qui est abord√© dans cette section n'est pas la configuration d'Istio, mais plut√¥t la configuration du tableau de bord du K8s. C'est tr√®s bien de voir les m√©triques et de voir l'√©tat du cluster (plus d'informations dans \[<https://istio.io/latest/docs/setup/platform>- setup/kind/#setup-dashboard-ui-for-kind)¬†:**](<https://istio.io/latest/docs/setup/platform-setup/kind/#setup-dashboard-ui-for-kind>)\*\*

1.  **Nous d√©ployons le d√©ploiement du tableau de bord¬†:**

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommende d.yaml --validate=false
```

2.  \*\*Nous v√©rifions que le pod est disponible et qu'il a √©t√© correctement cr√©√©¬†:

```bash
kubectl get pod -n kubernetes-dashboard
```

3.  **Nous cr√©ons un ServiceAccount et un ClusterRoleBinding pour donner un acc√®s administrateur au cluster¬†:**

```bash
kubectl create serviceaccount -n kubernetes-dashboard admin-user

kubectl create clusterrolebinding  -n kubernetes-dashboard admin-user  --clusterrole cluster-admin --serviceaccount=kubernetes-dashboard:admin-user
```

4.  **Nous g√©n√©rons le token qui nous demandera ensuite de nous connecter¬†:**

```bash
token=$(kubectl  -n  kubernetes-dashboard  describe  secret  $(kubectl  -n  kubernetes - dashboard get secret | awk '/^admin-user/{print $1}') | awk '$1=="token:"{print $2}')
```

5.  **On v√©rifie qu'il a √©t√© correctement stock√© dans la variable token : echo $token**
6.  **Nous pouvons acc√©der au tableau de bord via CLI en √©crivant¬†:**

```bash
kubectl proxy
```

**Nous pouvons d√©sormais acc√©der depuis le navigateur √† l'adresse \[http&#x3A;//localhost:8001/api/v1/namespaces/kubernetes - Dashboard/services/https&#x3A;kubernetes-dashboard:/proxy/#/login**](http&#x3A;//localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https&#x3A;kubernetes-dashboard:/proxy/#/login)\*\*

7.  **On acc√®de au tableau de bord du cluster depuis le web, une fois le token renseign√© (ex. Services, etc.) :**

**------------------------------------------------ JENKINS - -----------------------------------------**

1.  **Pour compl√©ter le cluster, nous allons √©galement pr√©senter Jenkins. Nous allons cr√©er un service de type NodePort mapp√© sur hostPort 30001, comme nous le verrons. Le fichier yaml qui contient le d√©ploiement et le service est¬†:**

**jenkins.yaml¬†:**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: jenkins
spec:
  type: NodePort
  selector:
    app: jenkins
  ports:
    - name: http
      port: 8080
      targetPort: 8080
      nodePort: 30001
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jenkins
spec:
  selector:
    matchLabels:
      app: jenkins
  replicas: 1
  template:
    metadata:
      labels:
        app: jenkins
    spec:
      containers:
        - name: jenkins
          image: jenkins/jenkins:lts
          ports:
            - containerPort: 8080
            - containerPort: 50000
```

2.  **√Ä ce stade, nous nous demanderons o√π nous allons exposer Jenkins. L'id√©e serait de le faire via nginx dans ¬´ location jenkins/ ¬ª et d'y acc√©der via localhost:30000/jenkins - il vous suffirait de modifier nginx-conf.yaml et d'ajouter**


    location /jenkins {

        proxy\_pass http://<jenkinsPodUsedIp>:30001/;
        proxy\_set\_header Host $host;
        proxy\_set\_header X-Real-IP $remote\_addr;
        proxy\_set\_header X-Forwarded-For $proxy\_add\_x\_forwarded\_for; proxy\_set\_header X-Forwarded-Proto $scheme;

    }

_Dans mon cas, je ne peux pas le faire comme √ßa en raison de probl√®mes de compatibilit√© avec wsl2. Pour ce faire, j'ai modifi√© le fichier cluster-config.yaml, en √©crivant les extraPortMappings dont nous aurons besoin plus tard. De plus, j'ai ajout√© quelques travailleurs et ajout√© une balise dans le ma√Ætre pour ajouter un contr√¥leur d'entr√©e plus tard¬†:_

**cluster-config.yaml**

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
  - containerPort: 443
    hostPort: 443
    protocol: TCP
  - containerPort: 8080
    hostPort: 8080
    protocol: TCP
  - containerPort: 30000
    hostPort: 30000
    protocol: TCP
  - containerPort: 30001
    hostPort: 30001
    protocol: TCP
- role: worker
- role: worker
```

3.  **Maintenant, vous pouvez continuer avec votre cluster et ajouter Jenkins. Si √† un moment donn√© vous vous √™tes perdu, il existe un fichier appel√© autoDeploy.sh qui vous montre l'int√©gralit√© du sc√©nario. Commentez les parties que vous n'avez pas atteintes afin de pouvoir essayer de les faire vous-m√™me et ex√©cuter¬†:**

```bash
./autoDeploy.sh
```

4.  **Enfin, nous pouvons acc√©der √† nginx via localhost:30000 et Jenkins via localhost:30001. Nous savons d√©j√† comment fonctionne nginx, passons √† Jenkins¬†:**

-   _D√©bloquez Jenkins¬†:_**nous avons besoin du mot de passe initial. Nous ex√©cutons :**

```bash
kubectl get pods
kubectl exec <nombre-pod-jenkins> -- cat** /var/jenkins\_home/secrets/initialAdminPassword**
```

-   **Nous allons sur localhost:30001 et entrons le mot de passe initial.**

-   **Installez les plugins recommand√©s, cr√©ez votre utilisateur administrateur et c'est pr√™t¬†!**

**NOTES¬†:**

-   **Si vous souhaitez utiliser un nom plus attractif que ¬´ localhost ¬ª, vous pouvez modifier le fichier /etc/hosts sur votre machine en ajoutant la ligne 127.0.0.1<nombre-de-dominio-guay>. Vous obtiendriez quelque chose comme ceci¬†:**

-   **Il est toujours conseill√© de travailler avec diff√©rents espaces de noms (par exemple pre, pro, dev, etc.). Nous ne le faisons pas dans cette br√®ve pratique pour ne pas compliquer davantage les choses, mais la meilleure chose √† faire serait de s√©parer Jenkins des autres services, etc.**

**----------------------------------------- CE -------- -----------------------------**

**Nous passons √† Istio. De par mon peu d'exp√©rience et la documentation que j'ai lue, Istio pose beaucoup de probl√®mes pour travailler avec un cluster local puisque, entre autres, nous ne g√©rons pas le trafic venant de l'ext√©rieur. Les configurations que nous allons faire ne sont donc pas excessivement complexes, mais plut√¥t illustratives de l‚Äôutilit√© du service.**

**Avant tout:**

-   _La ¬´¬†Gateway¬†¬ª est un composant Istio qui fait office de point d'entr√©e pour le trafic externe entrant dans le cluster._
-   _Le ¬´ VirtualService ¬ª est un objet Istio qui vous permet de d√©finir la mani√®re dont le trafic doit √™tre dirig√© de la passerelle vers les services back-end du cluster._

10. **Installation d'Istio en cluster¬†:**

-   **T√©l√©chargez et installez Istio¬†:**

```bash
curl -L https://istio.io/downloadIstio | sh -
cd istio-*
export PATH=$PWD/bin:$PATH
```

-   **V√©rifiez que les CRD sont install√©s avec¬†:**

```bash
kubectl get crds | grep 'istio.io\|certmanager.k8s.io' | wc -l
```

-   **Si nous obtenons 0, ex√©cutez¬†:**

```bash
istioctl install --set profile=default
```

-   **V√©rifiez que tout fonctionne et qu'une passerelle d'entr√©e et un pod istio ont √©t√© cr√©√©s¬†:**

```bash
kubectl get pods -n istio-system
```

-   **Activez Istio pour l'espace de noms dans lequel les pods Apache, Nginx et Jenkins sont d√©ploy√©s¬†:**

```bash
kubectl label namespace default istio-injection=enabled
```

11. **Pour effectuer une configuration simple d'Istio dans ce sc√©nario, nous pourrions ajouter un √©quilibreur de charge pour le service nginx √† l'aide du contr√¥le du trafic Istio. Pour cela:**

-   **G√©n√©rez un √©quilibreur de charge pour le service nginx. Nous cr√©ons un VirtualService et une Gateway dans Istio pour nginx¬†:**

**loadBalancerNginx.yaml¬†:**

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: nginx-vs
spec:
  hosts:
  - "*"
  gateways:
  - istio-gateway
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        host: nginx
        port:
          number: 80
---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: istio-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"

```

```bash
kubectl apply -f loadBalancerNginx.yaml
```

**NOTES¬†:**

-   **Th√©oriquement, nous devrions pouvoir acc√©der √† nginx via http&#x3A;//<nodeIP>:30000. En r√©alit√©, travailler sur du type local nous rend les choses plus difficiles puisque notre environnement ne prend pas en charge les √©quilibreurs de charge externes. Cela ne peut pas √™tre r√©solu sans modifier le fichier de configuration de kubelet, la configuration du cluster, etc. Si un √©quilibreur externe √©tait pris en charge, la proc√©dure serait de modifier les exportations comme suit (dans mon cas pour wsl2)¬†:**

```bash
export INGRESS_NAME=istio-ingressgateway
export INGRESS_NS=istio-system

export INGRESS_HOST=127.0.0.1

#check
kubectl get svc -n istio-system
echo "INGRESS_HOST=$INGRESS_HOST, INGRESS_PORT=$INGRESS_PORT"

export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
```

**Le fichier nginx-nodeport.yaml, au cas o√π vous voudriez vous amuser avec, est¬†: nginx-nodeport.yaml¬†:**

```bash
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: nginx-nodeport-vs
spec:
  hosts:
  - "*"
  gateways:
  - istio-gateway
  http:
  - match:
    - port: 30000
    route:
    - destination:
        host: nginx
        port:
          number: 80
```

-   **Il est possible que vous deviez red√©marrer les d√©ploiements (ou supprimer les pods) pour qu'ils aient inject√© istio¬†:**

```bash
kubectl rollout restart deployment nginx
kubectl rollout restart deployment apache
kubectl delete pods --all**
```

-   **V√©rifiez que tout fonctionne bien avec :**

```bash
istioctl analyze
kubectl get gateway
kubectl get vs
```

**--------------------------------------- KIALI ---------- -------------------------------**

**En supposant que nous disposons d√©j√† d'un √©quilibreur de charge qui redirige tout vers le proxy inverse nginx, nous passons √† l'outil suivant. Kiali est un outil de suivi/tableau de bord Istio qui fournit des visualisations et des analyses de la topologie du service Istio et des informations sur le trafic r√©seau. Il est g√©n√©ralement utilis√© chaque fois qu'Istio est configur√© (comme Grafana si Prometheus est configur√© ou Kibana si Elastic-Search est configur√©). Installons-le¬†:**

12. **Installateur¬†:**

```bash
kubectl  apply  -f  https://raw.githubuserconten 1.17/samples/addons/kiali.yaml --validate=false**
```

13. **V√©rifiez l'installation¬†:**

```bash
kubectl -n istio-system get svc kiali
```

14. **Utilisez le tableau de bord¬†:**

```bash
istioctl dashboard kiali
```

**CONCLUSION:**

**Istio est un service tr√®s complet qui peut √™tre int√©gr√© √† Prometheus, au tableau de bord Kubernetes lui-m√™me et √† de nombreux autres services et plateformes. Nous am√©liorerons les configurations d'Istio dans les prochains laboratoires o√π, entre autres, nous installerons Prometheus via Istio et nous concentrerons davantage sur la surveillance des clusters.**

**NOTES IMPORTANTES:**

-   L'ensemble du sc√©nario peut √™tre assembl√© en ex√©cutant le fichier autoDeploy.py, en cas de probl√®me. Le fichier accepte une seule entr√©e, qui est le nom du cluster. Si vous souhaitez le laisser par d√©faut, le nom est ¬´ trainingPath ¬ª (appuyez sur Entr√©e).

```bash
python3 autoDeploy.py
```

-   Si vous utilisez wsl2 ou une distribution Linux, il est pr√©f√©rable d'utiliser le fichier autoDeploy.sh, car il √©vite d'√©ventuels probl√®mes avec les exportations Python. Dans un r√©pertoire vide vous devez ex√©cuter :

```bash
./autoDeploy.sh
```

-   Il est important de modifier la version d'Istio √† installer (derni√®re version<https://istio.io/downloadIstio>)

**R√©f√©rences Istio pertinentes¬†:**

-   <https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-control/#using-node-ports-of-the-ingress-gateway-service>

-   <https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-control/#using-node-ports-of-the-ingress-gateway-service>

-   <https://istio.io/latest/docs/examples/bookinfo/#determine-the-ingress-ip-and-port>

-   <https://istio.io/latest/docs/examples/bookinfo/#determine-the-ingress-ip-and-port>
