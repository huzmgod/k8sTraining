# k8sFormation

Apache, Nginx, Jenkins, Istio, Kiali (pour le moment)

VÃ©rifier**_TrainingPathProject.pdf_**pour un fichier Lisez-moi complet ğŸ˜Š

Pour dÃ©ployer l'ensemble du scÃ©nario, exÃ©cutezÂ :

```bash
./autoDeploy.sh
```

Si vous prÃ©fÃ©rez jouer lentement, il suffit deÂ :

```bash
git clone https://github.com/huzmgod/k8sTraining.git
cd k8sTraining
```

**_-----------------------------------------------------------------------------------------------------------------------_**

**_README complet en espagnol (sans les images, qui sont dans le pdf)Â :_**

**Configuration de Nginx en tant que proxy inverse pour Apache Ã  l'aide de K8. Jenkins, Istio, K8s Dashboard et Kiali dÃ©ployÃ©s**

1.  **Type d'installationÂ :<https://kind.sigs.k8s.io/docs/user/quick-start/#installation>**
2.  **CrÃ©ez le fichier de configuration du cluster (cluster-config.yaml)Â :**

cluster-config.yaml

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
 extraPortMappings:
 - containerPort: 30000
   hostPort: 30000
   protocol: TCP
```

**NOTESÂ :**

-   L'extraPortMappings est important pour les environnements qui utilisent Docker Desktop, car sinon la configuration kind empÃªche l'accÃ¨s ultÃ©rieur au contenu du serveur Apache (ou autre) via localhost:30000 (ou un autre port supÃ©rieur Ã  30000). Pour plus de documentationÂ :<https://kind.sigs.k8s.io/docs/user/configuration/#extra-port-mappings>\*\*

-   Les travailleurs pourraient Ãªtre ajoutÃ©s avec la ligne

```yaml
-role: worker
```

3.  **CrÃ©ez le cluster et vÃ©rifiez l'Ã©tatÂ :**

```bash
kind create cluster --config=cluster kubectl cluster-info
kubectl cluster-info
```

4.  **Fichiers de configuration des pods (Apache et Nginx)Â :**

**UTILISER:**Le service et le dÃ©ploiement sont dans le mÃªme yaml, Ã  enregistrer sur kubectl apply -f. Vous pouvez Ã©galement utiliser kustomize pour gagner du temps de dÃ©ploiement.

**nginx.yamlÂ :**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: nginx
spec:
  type: NodePort
  ports:
  - name: http
    port: 80
    protocol: TCP
    targetPort: 80
    nodePort: 30000
  selector:
    app: nginx
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx
spec:
  selector:
    matchLabels:
      app: nginx
  replicas: 1
  template:
    metadata:
      labels:
        app: nginx
    spec:
      containers:
      - name: nginx
        image: nginx:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: nginx-conf
          mountPath: /etc/nginx/
      volumes:
      - name: nginx-conf
        configMap:
          name: nginx-conf
```

apache.yamlÂ :

```yaml

apiVersion: v1
kind: Service
metadata:
  name: apache
  labels:
    app: apache
spec:
  ports:
  - name: http
    port: 8080
    targetPort: 80
  selector:
    app: apache
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: apache
spec:
  selector:
    matchLabels:
      app: apache
  replicas: 1
  template:
    metadata:
      labels:
        app: apache
    spec:
      containers:
      - name: apache
        image: httpd:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: html
          mountPath: /usr/local/apache2/htdocs/
      volumes:
      - name: html
        configMap:
          name: html
```

5.  **CrÃ©ez le ConfigMap pour configurer nginx en tant que proxy inverse (nginx-conf.yaml)**

```yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: nginx-conf
data:
  nginx.conf: |
    events {
      worker_connections  1024;
    }

    http {
      server {
        listen 80;
        server_name localhost;

        location / {
          index index.html;
          proxy_pass http://apache:8080/;
          proxy_set_header Host $host;
          proxy_set_header X-Real-IP $remote_addr;
          proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }
      }
    }
```

**NOTESÂ :**

-   _La directive sur les travailleurs_connexions contrÃ´le le nombre maximum de connexions simultanÃ©es pouvant Ãªtre gÃ©rÃ©es par un processus de travail Nginx. Il doit Ãªtre adaptÃ© au trafic, 1024 en est un exemple._
-   _Le port Apache 80 est mappÃ© sur 8080 pour Ã©viter les conflits avec le pod nginx._

6.  **Appliquez les configurations au clusterÂ :**

```bash
kubectl apply -f nginx.yaml
kubectl apply -f apache.yaml
kubectl apply -f nginx-conf.yaml
```

On vÃ©rifie que tout va bien

```bash
kubectl get pods
kubectl exec <nombre de tu pod de nginx> -- cat /etc/nginx/nginx.conf
```

7.  **Si nous avons du HTML personnalisÃ©, nous pouvons l'utiliser pour le visualiser via nginx et nous assurer que tout fonctionne bien jusqu'Ã  prÃ©sent.**

_Les fichiers que je laisse s'appellent custom.html et custom.css, mais ils peuvent Ãªtre n'importe quel autre. C'est un HTML Dragon Ball qui est plutÃ´t cool._

-   **Nous crÃ©ons le ConfigMap Ã  partir des fichiers html et cssÂ :**

```bash
kubectl create configmap html --from-file=custom.html --from-file=custom.css
```

REMARQUEÂ : Le fichier html.yaml est Ã©galement laissÃ© en tant que ConfigMap Ã  appliquer avec kubectl

```bash
apply -f html.yaml
```

_C'est Ã©quivalent Ã  la commande prÃ©cÃ©dente._

-   **Nous mettons Ã  jour le fichier apache.yaml, en crÃ©ant un point de montage et en spÃ©cifiant le volume Ã  utiliserÂ :**

```yaml
...
spec:
      containers:
      - name: apache
        image: httpd:latest
        ports:
        - containerPort: 80
        volumeMounts:
        - name: html
          mountPath: /usr/local/apache2/htdocs/
      volumes:
      - name: html
        configMap:
          name: html
```

-   **Nous appliquons les configurations :**

```bash
kubectl apply -f apache.yaml
```

-   **On vÃ©rifie que tout a Ã©tÃ© chargÃ© et que le reverse proxy fonctionne correctement :**

```bash
kubectl exec <nombre de tu pod de nginx> -- curl localhost/custom.html
```

8.  **Ã€ prÃ©sent, nous devrions avoir configurÃ© correctement cela.**

**Et via localhost:30000, nous accÃ©derions Ã  notre page Web custom.html**

9.  **Si tout sâ€™est bien passÃ© jusquâ€™Ã  prÃ©sent, tant mieux, câ€™est comme Ã§a que Ã§a devrait Ãªtre. Sinon, j'ai laissÃ© un fichier kustomization.yaml qui utilise kustomize pour tout construire automatiquement. Si quelque chose ne va pas, suivez les Ã©tapes suivantesÂ :**

```bash
kind delete cluster â€“-name=<nombreDeTuCluster>
kind create cluster --config=cluster-config.yaml --name=<nombre-que-quieras>
kubectl apply -k .
```

**Tout prÃªt. Effectuez les vÃ©rifications ci-dessus (nginx.conf, localhost:30000, etc.) REMARQUEÂ :**

-   **Attention au copier-coller, les scripts ne se copient pas toujours bien.**
-   **Je sais que Ã§a aurait Ã©tÃ© bien plus tÃ´t, mais il faut tout reprendre depuis le dÃ©but**ğŸ˜Š**.**

**---------------------------------- TABLEAU DE BORD K8S -------------- --------------------------------**

**Ce qui est abordÃ© dans cette section n'est pas la configuration d'Istio, mais plutÃ´t la configuration du tableau de bord du K8s. C'est trÃ¨s bien de voir les mÃ©triques et de voir l'Ã©tat du cluster (plus d'informations dans \[<https://istio.io/latest/docs/setup/platform>- setup/kind/#setup-dashboard-ui-for-kind)Â :**](<https://istio.io/latest/docs/setup/platform-setup/kind/#setup-dashboard-ui-for-kind>)\*\*

1.  **Nous dÃ©ployons le dÃ©ploiement du tableau de bordÂ :**

```bash
kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.7.0/aio/deploy/recommende d.yaml --validate=false
```

2.  \*\*Nous vÃ©rifions que le pod est disponible et qu'il a Ã©tÃ© correctement crÃ©Ã©Â :

```bash
kubectl get pod -n kubernetes-dashboard
```

3.  **Nous crÃ©ons un ServiceAccount et un ClusterRoleBinding pour donner un accÃ¨s administrateur au clusterÂ :**

```bash
kubectl create serviceaccount -n kubernetes-dashboard admin-user

kubectl create clusterrolebinding  -n kubernetes-dashboard admin-user  --clusterrole cluster-admin --serviceaccount=kubernetes-dashboard:admin-user
```

4.  **Nous gÃ©nÃ©rons le token qui nous demandera ensuite de nous connecterÂ :**

```bash
token=$(kubectl  -n  kubernetes-dashboard  describe  secret  $(kubectl  -n  kubernetes - dashboard get secret | awk '/^admin-user/{print $1}') | awk '$1=="token:"{print $2}')
```

5.  **On vÃ©rifie qu'il a Ã©tÃ© correctement stockÃ© dans la variable token : echo $token**
6.  **Nous pouvons accÃ©der au tableau de bord via CLI en Ã©crivantÂ :**

```bash
kubectl proxy
```

**Nous pouvons dÃ©sormais accÃ©der depuis le navigateur Ã  l'adresse \[http&#x3A;//localhost:8001/api/v1/namespaces/kubernetes - Dashboard/services/https&#x3A;kubernetes-dashboard:/proxy/#/login**](http&#x3A;//localhost:8001/api/v1/namespaces/kubernetes-dashboard/services/https&#x3A;kubernetes-dashboard:/proxy/#/login)\*\*

7.  **On accÃ¨de au tableau de bord du cluster depuis le web, une fois le token renseignÃ© (ex. Services, etc.) :**

**------------------------------------------------ JENKINS - -----------------------------------------**

1.  **Pour complÃ©ter le cluster, nous allons Ã©galement prÃ©senter Jenkins. Nous allons crÃ©er un service de type NodePort mappÃ© sur hostPort 30001, comme nous le verrons. Le fichier yaml qui contient le dÃ©ploiement et le service estÂ :**

**jenkins.yamlÂ :**

```yaml
apiVersion: v1
kind: Service
metadata:
  name: jenkins
spec:
  type: NodePort
  selector:
    app: jenkins
  ports:
    - name: http
      port: 8080
      targetPort: 8080
      nodePort: 30001
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: jenkins
spec:
  selector:
    matchLabels:
      app: jenkins
  replicas: 1
  template:
    metadata:
      labels:
        app: jenkins
    spec:
      containers:
        - name: jenkins
          image: jenkins/jenkins:lts
          ports:
            - containerPort: 8080
            - containerPort: 50000
```

2.  **Ã€ ce stade, nous nous demanderons oÃ¹ nous allons exposer Jenkins. L'idÃ©e serait de le faire via nginx dans Â« location jenkins/ Â» et d'y accÃ©der via localhost:30000/jenkins - il vous suffirait de modifier nginx-conf.yaml et d'ajouter**


    location /jenkins {

        proxy\_pass http://<jenkinsPodUsedIp>:30001/;
        proxy\_set\_header Host $host;
        proxy\_set\_header X-Real-IP $remote\_addr;
        proxy\_set\_header X-Forwarded-For $proxy\_add\_x\_forwarded\_for; proxy\_set\_header X-Forwarded-Proto $scheme;

    }

_Dans mon cas, je ne peux pas le faire comme Ã§a en raison de problÃ¨mes de compatibilitÃ© avec wsl2. Pour ce faire, j'ai modifiÃ© le fichier cluster-config.yaml, en Ã©crivant les extraPortMappings dont nous aurons besoin plus tard. De plus, j'ai ajoutÃ© quelques travailleurs et ajoutÃ© une balise dans le maÃ®tre pour ajouter un contrÃ´leur d'entrÃ©e plus tardÂ :_

**cluster-config.yaml**

```yaml
kind: Cluster
apiVersion: kind.x-k8s.io/v1alpha4
nodes:
- role: control-plane
  kubeadmConfigPatches:
  - |
    kind: InitConfiguration
    nodeRegistration:
      kubeletExtraArgs:
        node-labels: "ingress-ready=true"
  extraPortMappings:
  - containerPort: 80
    hostPort: 80
    protocol: TCP
  - containerPort: 443
    hostPort: 443
    protocol: TCP
  - containerPort: 8080
    hostPort: 8080
    protocol: TCP
  - containerPort: 30000
    hostPort: 30000
    protocol: TCP
  - containerPort: 30001
    hostPort: 30001
    protocol: TCP
- role: worker
- role: worker
```

3.  **Maintenant, vous pouvez continuer avec votre cluster et ajouter Jenkins. Si Ã  un moment donnÃ© vous vous Ãªtes perdu, il existe un fichier appelÃ© autoDeploy.sh qui vous montre l'intÃ©gralitÃ© du scÃ©nario. Commentez les parties que vous n'avez pas atteintes afin de pouvoir essayer de les faire vous-mÃªme et exÃ©cuterÂ :**

```bash
./autoDeploy.sh
```

4.  **Enfin, nous pouvons accÃ©der Ã  nginx via localhost:30000 et Jenkins via localhost:30001. Nous savons dÃ©jÃ  comment fonctionne nginx, passons Ã  JenkinsÂ :**

-   _DÃ©bloquez JenkinsÂ :_**nous avons besoin du mot de passe initial. Nous exÃ©cutons :**

```bash
kubectl get pods
kubectl exec <nombre-pod-jenkins> -- cat** /var/jenkins\_home/secrets/initialAdminPassword**
```

-   **Nous allons sur localhost:30001 et entrons le mot de passe initial.**

-   **Installez les plugins recommandÃ©s, crÃ©ez votre utilisateur administrateur et c'est prÃªtÂ !**

**NOTESÂ :**

-   **Si vous souhaitez utiliser un nom plus attractif que Â« localhost Â», vous pouvez modifier le fichier /etc/hosts sur votre machine en ajoutant la ligne 127.0.0.1<nombre-de-dominio-guay>. Vous obtiendriez quelque chose comme ceciÂ :**

-   **Il est toujours conseillÃ© de travailler avec diffÃ©rents espaces de noms (par exemple pre, pro, dev, etc.). Nous ne le faisons pas dans cette brÃ¨ve pratique pour ne pas compliquer davantage les choses, mais la meilleure chose Ã  faire serait de sÃ©parer Jenkins des autres services, etc.**

**----------------------------------------- CE -------- -----------------------------**

**Nous passons Ã  Istio. De par mon peu d'expÃ©rience et la documentation que j'ai lue, Istio pose beaucoup de problÃ¨mes pour travailler avec un cluster local puisque, entre autres, nous ne gÃ©rons pas le trafic venant de l'extÃ©rieur. Les configurations que nous allons faire ne sont donc pas excessivement complexes, mais plutÃ´t illustratives de lâ€™utilitÃ© du service.**

**Avant tout:**

-   _La Â«Â GatewayÂ Â» est un composant Istio qui fait office de point d'entrÃ©e pour le trafic externe entrant dans le cluster._
-   _Le Â« VirtualService Â» est un objet Istio qui vous permet de dÃ©finir la maniÃ¨re dont le trafic doit Ãªtre dirigÃ© de la passerelle vers les services back-end du cluster._

10. **Installation d'Istio en clusterÂ :**

-   **TÃ©lÃ©chargez et installez IstioÂ :**

```bash
curl -L https://istio.io/downloadIstio | sh -
cd istio-*
export PATH=$PWD/bin:$PATH
```

-   **VÃ©rifiez que les CRD sont installÃ©s avecÂ :**

```bash
kubectl get crds | grep 'istio.io\|certmanager.k8s.io' | wc -l
```

-   **Si nous obtenons 0, exÃ©cutezÂ :**

```bash
istioctl install --set profile=default
```

-   **VÃ©rifiez que tout fonctionne et qu'une passerelle d'entrÃ©e et un pod istio ont Ã©tÃ© crÃ©Ã©sÂ :**

```bash
kubectl get pods -n istio-system
```

-   **Activez Istio pour l'espace de noms dans lequel les pods Apache, Nginx et Jenkins sont dÃ©ployÃ©sÂ :**

```bash
kubectl label namespace default istio-injection=enabled
```

11. **Pour effectuer une configuration simple d'Istio dans ce scÃ©nario, nous pourrions ajouter un Ã©quilibreur de charge pour le service nginx Ã  l'aide du contrÃ´le du trafic Istio. Pour cela:**

-   **GÃ©nÃ©rez un Ã©quilibreur de charge pour le service nginx. Nous crÃ©ons un VirtualService et une Gateway dans Istio pour nginxÂ :**

**loadBalancerNginx.yamlÂ :**

```yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: nginx-vs
spec:
  hosts:
  - "*"
  gateways:
  - istio-gateway
  http:
  - match:
    - uri:
        prefix: /
    route:
    - destination:
        host: nginx
        port:
          number: 80
---
apiVersion: networking.istio.io/v1alpha3
kind: Gateway
metadata:
  name: istio-gateway
spec:
  selector:
    istio: ingressgateway
  servers:
  - port:
      number: 80
      name: http
      protocol: HTTP
    hosts:
    - "*"

```

```bash
kubectl apply -f loadBalancerNginx.yaml
```

**NOTESÂ :**

-   **ThÃ©oriquement, nous devrions pouvoir accÃ©der Ã  nginx via http&#x3A;//<nodeIP>:30000. En rÃ©alitÃ©, travailler sur du type local nous rend les choses plus difficiles puisque notre environnement ne prend pas en charge les Ã©quilibreurs de charge externes. Cela ne peut pas Ãªtre rÃ©solu sans modifier le fichier de configuration de kubelet, la configuration du cluster, etc. Si un Ã©quilibreur externe Ã©tait pris en charge, la procÃ©dure serait de modifier les exportations comme suit (dans mon cas pour wsl2)Â :**

```bash
export INGRESS_NAME=istio-ingressgateway
export INGRESS_NS=istio-system

export INGRESS_HOST=127.0.0.1

#check
kubectl get svc -n istio-system
echo "INGRESS_HOST=$INGRESS_HOST, INGRESS_PORT=$INGRESS_PORT"

export GATEWAY_URL=$INGRESS_HOST:$INGRESS_PORT
```

**Le fichier nginx-nodeport.yaml, au cas oÃ¹ vous voudriez vous amuser avec, estÂ : nginx-nodeport.yamlÂ :**

```bash
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: nginx-nodeport-vs
spec:
  hosts:
  - "*"
  gateways:
  - istio-gateway
  http:
  - match:
    - port: 30000
    route:
    - destination:
        host: nginx
        port:
          number: 80
```

-   **Il est possible que vous deviez redÃ©marrer les dÃ©ploiements (ou supprimer les pods) pour qu'ils aient injectÃ© istioÂ :**

```bash
kubectl rollout restart deployment nginx
kubectl rollout restart deployment apache
kubectl delete pods --all**
```

-   **VÃ©rifiez que tout fonctionne bien avec :**

```bash
istioctl analyze
kubectl get gateway
kubectl get vs
```

**--------------------------------------- KIALI ---------- -------------------------------**

**En supposant que nous disposons dÃ©jÃ  d'un Ã©quilibreur de charge qui redirige tout vers le proxy inverse nginx, nous passons Ã  l'outil suivant. Kiali est un outil de suivi/tableau de bord Istio qui fournit des visualisations et des analyses de la topologie du service Istio et des informations sur le trafic rÃ©seau. Il est gÃ©nÃ©ralement utilisÃ© chaque fois qu'Istio est configurÃ© (comme Grafana si Prometheus est configurÃ© ou Kibana si Elastic-Search est configurÃ©). Installons-leÂ :**

12. **InstallateurÂ :**

```bash
kubectl  apply  -f  https://raw.githubuserconten 1.17/samples/addons/kiali.yaml --validate=false**
```

13. **VÃ©rifiez l'installationÂ :**

```bash
kubectl -n istio-system get svc kiali
```

14. **Utilisez le tableau de bordÂ :**

```bash
istioctl dashboard kiali
```

**CONCLUSION:**

**Istio est un service trÃ¨s complet qui peut Ãªtre intÃ©grÃ© Ã  Prometheus, au tableau de bord Kubernetes lui-mÃªme et Ã  de nombreux autres services et plateformes. Nous amÃ©liorerons les configurations d'Istio dans les prochains laboratoires oÃ¹, entre autres, nous installerons Prometheus via Istio et nous concentrerons davantage sur la surveillance des clusters.**

**NOTES IMPORTANTES:**

-   L'ensemble du scÃ©nario peut Ãªtre assemblÃ© en exÃ©cutant le fichier autoDeploy.py, en cas de problÃ¨me. Le fichier accepte une seule entrÃ©e, qui est le nom du cluster. Si vous souhaitez le laisser par dÃ©faut, le nom est Â« trainingPath Â» (appuyez sur EntrÃ©e).

```bash
python3 autoDeploy.py
```

-   Si vous utilisez wsl2 ou une distribution Linux, il est prÃ©fÃ©rable d'utiliser le fichier autoDeploy.sh, car il Ã©vite d'Ã©ventuels problÃ¨mes avec les exportations Python. Dans un rÃ©pertoire vide vous devez exÃ©cuter :

```bash
./autoDeploy.sh
```

-   Il est important de modifier la version d'Istio Ã  installer (derniÃ¨re version<https://istio.io/downloadIstio>)

**RÃ©fÃ©rences Istio pertinentesÂ :**

-   <https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-control/#using-node-ports-of-the-ingress-gateway-service>

-   <https://istio.io/latest/docs/tasks/traffic-management/ingress/ingress-control/#using-node-ports-of-the-ingress-gateway-service>

-   <https://istio.io/latest/docs/examples/bookinfo/#determine-the-ingress-ip-and-port>

-   <https://istio.io/latest/docs/examples/bookinfo/#determine-the-ingress-ip-and-port>
